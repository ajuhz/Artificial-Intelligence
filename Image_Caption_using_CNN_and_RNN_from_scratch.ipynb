{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Caption using CNN and RNN from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajuhz/Artificial-Intelligence/blob/master/Image_Caption_using_CNN_and_RNN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKVs_as3-ZfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e16fb99-9677-4646-818b-c966e3b3ce07"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGL2C9aG-f2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Projects/RNN/Python Proj/image-caption-pytorch-master')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBx0aYqiDcfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2de4c0c-6ffb-48fc-9ee3-5154e5893217"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Projects/RNN/Python Proj/image-caption-pytorch-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sov_cp5hCaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all necessary imports at first place\n",
        "import os, time, json, re\n",
        "import itertools, argparse, pickle, random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR, ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "import torchvision"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAJ15UumhLLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flikr8k(Dataset):\n",
        "\n",
        "    def __init__(self,data_part,data_preproc,transform):\n",
        "        self.path = data_preproc.path\n",
        "        self.data_part = data_part\n",
        "        self.captions, self.image_ids, self.idx_to_word, self.word_to_idx = \\\n",
        "            data_preproc.get_captions_ids_mapping(self.data_part)\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # feed_forward_net() defined in encode_image.py\n",
        "        caption = self.captions[index]\n",
        "        im_id = self.image_ids[index]\n",
        "        im = Image.open(self.path+'Flicker8k_Dataset//'+im_id)\n",
        "        if self.transform is not None:\n",
        "            im = self.transform(im)\n",
        "       \n",
        "        return im, torch.LongTensor(caption)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc7UUV2XhQNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_Preprocessing():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "    \n",
        "    def get_captions_ids_mapping(self,data_part, maxlen=40, threshold=1):\n",
        "        \n",
        "        # word_idx_map(), tokenize() defined in tokenize_caption.py\n",
        "        trn_files, test_files = self.split_image_files() #list of test and train images\n",
        "        trn_raw_captions, test_raw_captions = self.split_captions(trn_files, test_files)\n",
        "        idx_to_word, word_to_idx = self.word_idx_map(trn_raw_captions, threshold)\n",
        "        if data_part == 'train':\n",
        "            captions, image_ids = self.tokenize(trn_raw_captions, word_to_idx, maxlen)\n",
        "        if data_part == 'test':\n",
        "            captions, image_ids = self.tokenize(test_raw_captions, word_to_idx, maxlen)\n",
        "        return captions, image_ids, idx_to_word, word_to_idx\n",
        "    \n",
        "    \n",
        "    def split_captions(self,trn_files, test_files):\n",
        "        # load raw captions\n",
        "        raw_f = open(self.path + 'Flickr8k.token.txt', 'r').read().strip().split('\\n')\n",
        "        raw_captions = {}\n",
        "        for line in raw_f:\n",
        "            line = line.split('\\t')\n",
        "            im_id, cap = line[0][:len(line[0])-2], line[1]\n",
        "            if im_id not in raw_captions:\n",
        "                raw_captions[im_id] = ['<start> ' + cap + ' <end>']\n",
        "            else:\n",
        "                raw_captions[im_id].append('<start> ' + cap + ' <end>')\n",
        "        trn_raw_captions, test_raw_captions = {}, {}\n",
        "        for im_id in trn_files: trn_raw_captions[im_id] = raw_captions[im_id]\n",
        "        for im_id in test_files: test_raw_captions[im_id] = raw_captions[im_id]\n",
        "        return trn_raw_captions, test_raw_captions\n",
        "    \n",
        "    def split_image_files(self):\n",
        "        # load file names\n",
        "        im_files = os.listdir(self.path + 'Flicker8k_Dataset')\n",
        "        trn_files = open(self.path+'Flickr_8k.trainImages.txt', 'r').read().strip().split('\\n')\n",
        "        dev_files = open(self.path+'Flickr_8k.devImages.txt', 'r').read().strip().split('\\n')\n",
        "        test_files = open(self.path+'Flickr_8k.testImages.txt', 'r').read().strip().split('\\n')\n",
        "        trn_files += list(set(im_files) - set(trn_files) - set(dev_files) - set(test_files))\n",
        "        trn_files += dev_files\n",
        "        return trn_files, test_files\n",
        "    \n",
        "    \n",
        "    def word_idx_map(self,raw_captions, threshold):\n",
        "        caps = []\n",
        "        for im in raw_captions:\n",
        "            for s in raw_captions[im]:\n",
        "                caps.append(s.split())\n",
        "\n",
        "        word_freq = nltk.FreqDist(itertools.chain(*caps))\n",
        "        idx_to_word = ['<pad>'] + [word for word, cnt in word_freq.items() if cnt >= threshold] + ['<unk>']\n",
        "        word_to_idx = {word:idx for idx, word in enumerate(idx_to_word)}\n",
        "\n",
        "        return idx_to_word, word_to_idx\n",
        "    \n",
        "    def tokenize(self,captions, word_to_idx, maxlen):\n",
        "        '''\n",
        "        Inputs:\n",
        "        - captions: dictionary with image_id as key, captions as value\n",
        "        - word_to_idx: mapping from word to index\n",
        "        - maxlen: max length of each sequence of tokens\n",
        "\n",
        "        Returns:\n",
        "        - tokens: array of shape (data_size, maxlen)\n",
        "        - image_ids: list of length data_size, mapping token to corresponding image_id\n",
        "        '''\n",
        "        tokens, image_ids = [], []\n",
        "        for im_id in captions:\n",
        "            for cap in captions[im_id]:\n",
        "                token = [(lambda x: word_to_idx[x] if x in word_to_idx else word_to_idx['<unk>'])(w) \\\n",
        "                         for w in cap.split()]\n",
        "                if len(token) > maxlen:\n",
        "                    token = token[:maxlen]\n",
        "                else:\n",
        "                    token += [0] * (maxlen-len(token))\n",
        "                tokens.append(token)\n",
        "                image_ids.append(im_id)\n",
        "        return np.array(tokens).astype('int32'), np.array(image_ids)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_9yEU_YhVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_Loader():\n",
        "    def __init__(self ,data_preporc ,data_part,transforms=False):\n",
        "        self.path = data_preporc.path\n",
        "        self.data_preporc = data_preporc\n",
        "        self.transforms = transforms\n",
        "        self.data_part = data_part\n",
        "        \n",
        "    def prepare_loader(self,batch_size):\n",
        "        \n",
        "        if self.transforms:\n",
        "            transform = self.transf()\n",
        "        else:\n",
        "            transform = None\n",
        "        rgb_mean = [0.485, 0.456, 0.406]\n",
        "        rgb_std = [0.229, 0.224, 0.225]\n",
        "        if self.data_part == 'train':\n",
        "            data_set = Flikr8k(self.data_part,self.data_preporc, transform=transform['transform_train'])\n",
        "            loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
        "        else:\n",
        "            data_set = Flikr8k(self.data_part, self.data_preporc,transform=transform['transform'])\n",
        "            dset_sampler = sampler.SubsetRandomSampler(range(len(data_set)))\n",
        "            loader = DataLoader(data_set, batch_size=batch_size, sampler=dset_sampler)\n",
        "        return loader\n",
        "    \n",
        "    def transf (self):\n",
        "        rgb_mean = [0.485, 0.456, 0.406]\n",
        "        rgb_std = [0.229, 0.224, 0.225]\n",
        "        transform ={}\n",
        "        transform['transform_train'] = T.Compose([\n",
        "                        T.Resize((224, 224)),\n",
        "                        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                        T.RandomResizedCrop(224, scale=(0.75, 1.0)),\n",
        "                        T.RandomHorizontalFlip(),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize(rgb_mean, rgb_std),\n",
        "                    ])\n",
        "        transform['transform']= T.Compose([\n",
        "                        T.Resize((224, 224)),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize(rgb_mean, rgb_std),\n",
        "                    ])\n",
        "        return transform\n",
        "        \n",
        "        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pfp3777hYk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder_Resnet101(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Encoder_Resnet101, self).__init__()\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        module_layers = list(resnet.children())\n",
        "        selected_layers = module_layers[:-1] # removing last 2 layers which are linear classifcation and average pooling\n",
        "        self.resnet = nn.Sequential(*selected_layers)\n",
        "        #self.avgpool = nn.AvgPool2d(7) # as per the ResNet Architecture for input (batch_size,3,256,256) o/p of last layer will be (batch_size,2048,8,8)\n",
        "        # after average pooling shape will be (batch_size,2048,1,1)\n",
        "        self.fine_tune()\n",
        "    \n",
        "    def fine_tune(self,fine_tune=False):\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self,images):\n",
        "        \"\"\"\n",
        "        images size = (batch_size,3,224,224)\n",
        "        \"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "        global_features = self.resnet(images)\n",
        "        #global_features = self.avgpool(global_features) # (batch_size,2048,1,1)\n",
        "        #global_features = global_features.view(batch_size,-1) #(batch_size,2048)\n",
        "        return global_features"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXj8S9trhbjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Designing Decoder with LSTM for Image Caption\n",
        "\"\"\"\n",
        "Decoder_Lstm(\n",
        "  (bn_f): BatchNorm1d(2048, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
        "  (proj_f): Linear(in_features=2048, out_features=50, bias=True)\n",
        "  (proj_h): Linear(in_features=2048, out_features=160, bias=True)\n",
        "  (proj_c): Linear(in_features=2048, out_features=160, bias=True)\n",
        "  (relu): ReLU(inplace=True)\n",
        "  (dropout): Dropout(p=0.2, inplace=False)\n",
        "  (embed): Embedding(9080, 50, padding_idx=0)\n",
        "  (dropout_emb): Dropout(p=0.1, inplace=False)\n",
        "  (lstm): LSTM(50, 160, batch_first=True)\n",
        "  (linear): Linear(in_features=160, out_features=9080, bias=True)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "class Decoder_Lstm(nn.Module):\n",
        "    def __init__(self, input_dim,embedding_dim, hidden_dim,vocab_size, num_layers=1, batch_first=True):\n",
        "        super(Decoder_Lstm,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.input_dim=input_dim\n",
        "        #all layers\n",
        "        self.bn_f = nn.BatchNorm1d(self.input_dim, momentum=0.01)\n",
        "        self.proj_f = nn.Linear(self.input_dim, self.embedding_dim)\n",
        "        self.proj_h = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.proj_c = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.dropout_emb = nn.Dropout(p=0.1)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "        \n",
        "        ### additional lines for better performance\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, np.sqrt(2./m.in_features))\n",
        "        \n",
        "        \n",
        "    def forward(self, im_feat, word_seq,print_flag=False):\n",
        "        #step 1\n",
        "        im_feat = self.bn_f(im_feat)\n",
        "        if print_flag:\n",
        "            print('after #step 1: im_feat.shape: {}'.format(im_feat.shape))\n",
        "        #step 2\n",
        "        im_hid = self.relu(self.proj_h(im_feat)).unsqueeze(0)\n",
        "        if print_flag:\n",
        "            print('after #step 2: im_hid.shape: {}'.format(im_hid.shape))\n",
        "        #step 3\n",
        "        im_state = self.relu(self.proj_c(im_feat)).unsqueeze(0)\n",
        "        if print_flag:\n",
        "            print('after #step 3: im_state.shape: {}'.format(im_state.shape))\n",
        "        #step 4 (this cat will be useful once we pass features to all steps)\n",
        "        h0 = torch.cat((im_hid, ) * self.num_layers, 0)\n",
        "        if print_flag:\n",
        "            print('after #step 4: h0.shape: {}'.format(h0.shape))\n",
        "        #step 5(this cat will be useful once we pass features to all steps)\n",
        "        c0 = torch.cat((im_state, ) * self.num_layers, 0)\n",
        "        if print_flag:\n",
        "            print('after #step 5: c0.shape: {}'.format(c0.shape))\n",
        "        #step 6\n",
        "        hidden=(h0, c0)\n",
        "        \n",
        "        #step 7\n",
        "        im_feat = self.relu(self.proj_f(im_feat))\n",
        "        if print_flag:\n",
        "            print('after #step 7: im_feat.shape: {}'.format(im_feat.shape))\n",
        "        #step 8\n",
        "       \n",
        "        #step 9\n",
        "        emb = self.embed(word_seq)\n",
        "        if print_flag:\n",
        "            print('after #step 9: word_seq.shape: {}'.format(word_seq.shape))\n",
        "        #step 10\n",
        "        emb = self.dropout_emb(emb)\n",
        "        if print_flag:\n",
        "            print('after #step 10: emb.shape: {}'.format(emb.shape))\n",
        "         #step 11\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        if print_flag:\n",
        "            print('after #step 11: output.shape: {}'.format(output.shape))\n",
        "         #step 12\n",
        "        scores = self.linear(output)\n",
        "        if print_flag:\n",
        "            print('after #step 12: output.shape: {}'.format(output.shape))\n",
        "         #step 13\n",
        "        return scores,hidden"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMSS0t4chgIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train function\n",
        "\n",
        "class Train_Model():\n",
        "    def __init__(self,dataset_loader,**kwargs):\n",
        "        # hyperparameters\n",
        "        self.lr_init = kwargs.pop('lr_init', 0.01)\n",
        "        self.lr_decay = kwargs.pop('lr_decay', 0.1)\n",
        "        self.step_size = kwargs.pop('step_size', 16)\n",
        "        self.batch_size = kwargs.pop('batch_size', 32)\n",
        "        self.print_every = kwargs.pop('print_every', 200)\n",
        "        self.checkpoint_name = kwargs.pop('checkpoint_name', 'im_caption')\n",
        "        #Data variables\n",
        "        self.data = dataset_loader.dataset\n",
        "        self.vocab_size = len(self.data.idx_to_word)\n",
        "        self._pad = self.data.word_to_idx['<pad>']\n",
        "        self._start = self.data.word_to_idx['<start>']\n",
        "        self._end = self.data.word_to_idx['<end>']\n",
        "        self.dataset_loader = dataset_loader\n",
        "        \n",
        "\n",
        "        #Model variables\n",
        "        self.model = Decoder_Lstm(2048, 50, 160, vocab_size, num_layers=1)\n",
        "        self.encoder = Encoder_Resnet101()\n",
        "        self.encoder.eval()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr_init,momentum=0.9, weight_decay=0.001)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=lr_decay)\n",
        "        self.model = self.model.to(device=device)\n",
        "        self._reset()\n",
        "        \n",
        "        \n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Set up some book-keeping variables for optimization. Don't call this\n",
        "        manually.\n",
        "        \"\"\"\n",
        "        self.best_val_loss = 999.\n",
        "        self.best_val_bleu = 0.\n",
        "        self.loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.bleu_history = []\n",
        "        self.val_bleu_history = []\n",
        "        \n",
        "    def _save_checkpoint(self, epoch, train_loss):\n",
        "        torch.save(self.model.state_dict(),\n",
        "            output_path+self.checkpoint_name+'_%.3f_epoch_%d.pth.tar' %(train_loss,epoch))\n",
        "        checkpoint = {\n",
        "            'optimizer': str(type(self.optimizer)),\n",
        "            'scheduler': str(type(self.scheduler)),\n",
        "            'lr_init': self.lr_init,\n",
        "            'lr_decay': self.lr_decay,\n",
        "            'step_size': self.step_size,\n",
        "            'batch_size': self.batch_size,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        with open(output_path+'hyper_param_optim.json', 'w') as f:\n",
        "            json.dump(checkpoint, f)    \n",
        "\n",
        "    def train(self,epochs,batch_size):\n",
        "        self.encoder = self.encoder.to(device)\n",
        "        self.model = self.model.to(device)\n",
        "        t0 = time.time()\n",
        "        train_loader= self.dataset_loader\n",
        "        for e in range(epochs):\n",
        "                       \n",
        "            print('\\nEpoch %d / %d:' % (e + 1, epochs))\n",
        "            self.model.train()\n",
        "\n",
        "           \n",
        "            running_loss = 0.\n",
        "\n",
        "            for t, (ims, captions) in enumerate(train_loader):\n",
        "                print('t :{} , loaders.image shape: {} dataloaders.captions shape:{}'.format(t, ims.shape, captions.shape))\n",
        "                with torch.no_grad():\n",
        "                    ims = ims.to(device)\n",
        "                    features = self.encoder(ims).squeeze()\n",
        "\n",
        "                features = features.to(device=device)\n",
        "                captions = captions.to(device=device)\n",
        "                mask = (captions[:, 1:] != self._pad).view(-1)\n",
        "                print('mask shape: {}'.format(mask.shape))\n",
        "\n",
        "                cap_input = captions[:, :-1]\n",
        "                cap_target = captions[:, 1:]\n",
        "\n",
        "                print('cap input shape: {}'.format(cap_input.shape))\n",
        "                print('cap target shape: {}'.format(cap_target.shape))\n",
        "                scores, hidden = self.model(features, cap_input)\n",
        "\n",
        "                print('scores shape: {} hidden shape: {}'.format(scores.shape,hidden[0].shape))\n",
        "                loss = F.cross_entropy(torch.reshape(scores, (-1, scores.size(2)))[mask],\n",
        "                                       torch.reshape(cap_target, (-1,))[mask],\n",
        "                                       size_average=False) / features.size(0)\n",
        "                if (t + 1) % print_every == 0:\n",
        "                    print('t = %d, loss = %.4f' % (t+1, loss.item()))\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * features.size(0)\n",
        "                #if t>0 :\n",
        "                #    break\n",
        "            if e%3==0:\n",
        "                self.scheduler.step()\n",
        "            N = len(self.data)\n",
        "            train_loss = running_loss / N\n",
        "            #train_bleu = self.check_bleu(train_loader, num_batches=1)\n",
        "            #val_loss, val_bleu = self.check_bleu(val_loader, num_batches=1, check_loss=True)\n",
        "\n",
        "        #self.scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        # Checkpoint and record/print metrics at epoch end\n",
        "        self.loss_history.append(train_loss)\n",
        "        #self.val_loss_history.append(val_loss)\n",
        "        #self.bleu_history.append(train_bleu)\n",
        "        #self.val_bleu_history.append(val_bleu)\n",
        "\n",
        "        # for floydhub metric graphs\n",
        "        #print('{\"metric\": \"BLEU.\", \"value\": %.4f, \"epoch\": %d}' % (train_bleu, e+1))\n",
        "        #print('{\"metric\": \"Val. BLEU.\", \"value\": %.4f, \"epoch\": %d}' % (val_bleu, e+1))\n",
        "        print('{\"metric\": \"Train Loss\", \"value\": %.4f, \"epoch\": %d}' % (train_loss, e+1))\n",
        "        #print('{\"metric\": \"Val. Loss\", \"value\": %.4f, \"epoch\": %d}' % (val_loss, e+1))\n",
        "\n",
        "#         if val_bleu > self.best_val_bleu:\n",
        "#             print('updating best val bleu...')\n",
        "#             self.best_val_bleu = val_bleu\n",
        "#             if e > 10:\n",
        "#                 print('Saving model...')\n",
        "#                 self._save_checkpoint(e+1, val_loss, val_bleu)\n",
        "#         elif val_loss < self.best_val_loss:\n",
        "#             print('updating best val loss...')\n",
        "#             self.best_val_loss = val_loss\n",
        "#             if e > 10:\n",
        "#                 print('Saving model...')\n",
        "#                 self._save_checkpoint(e+1, val_loss, val_bleu)\n",
        "#         print()\n",
        "        if e%10 == 0:\n",
        "                print('Saving model...')\n",
        "                self._save_checkpoint(e+1, train_loss, val_bleu)\n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print('Training complete in {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        \n",
        "        \n",
        "    def check_bleu(self, loader, num_batches, check_loss=False):\n",
        "\n",
        "        if check_loss:\n",
        "            self.model.eval()\n",
        "            loss = 0\n",
        "\n",
        "        total_score = 0.\n",
        "        with torch.no_grad():\n",
        "            for t, (ims, captions) in enumerate(loader):\n",
        "                ims = ims.to(device)\n",
        "                features = self.encoder(ims).squeeze()\n",
        "                gt_captions = self.decode_captions(captions.numpy(), self.data.idx_to_word)\n",
        "\n",
        "                if check_loss:\n",
        "                    \n",
        "                   # loss += self.forward_net(features, captions)\n",
        "                    features = features.to(device=device)\n",
        "                    captions = captions.to(device=device)\n",
        "                    mask = (captions[:, 1:] != self._pad).view(-1)\n",
        "\n",
        "                    cap_input = captions[:, :-1]\n",
        "                    cap_target = captions[:, 1:]\n",
        "                    scores, hidden = self.model(features, cap_input)\n",
        "                    loss_temp = F.cross_entropy(torch.reshape(scores, (-1, scores.size(2)))[mask],\n",
        "                                           torch.reshape(cap_target, (-1,))[mask],\n",
        "                                           size_average=False) / features.size(0)\n",
        "                    loss +=loss_temp\n",
        "\n",
        "                sample_captions = self.sample(features)\n",
        "                sample_captions = self.decode_captions(sample_captions, self.data.idx_to_word)\n",
        "                # for gt_caption, sample_caption in zip(gt_captions, sample_captions):\n",
        "                #     total_score += BLEU_score(gt_caption, sample_caption)\n",
        "                total_score += eval_bleu.calculate(list(map(self.remove_special_tokens, gt_captions)),\n",
        "                                    list(map(self.remove_special_tokens, sample_captions)))['bleu_1']\n",
        "\n",
        "                if (t+1) == num_batches:\n",
        "                    break\n",
        "\n",
        "        if check_loss:\n",
        "            loss /= num_batches\n",
        "            # return loss.item(), total_score / (num_batches*loader.batch_size)\n",
        "            return loss.item(), total_score / num_batches\n",
        "\n",
        "        # return total_score / (num_batches*loader.batch_size)\n",
        "        return total_score / num_batches\n",
        "    \n",
        "    def decode_captions(self,tokens, idx_to_word):\n",
        "        '''\n",
        "        Inputs:\n",
        "        - tokens: (N, ) or (N, T) array\n",
        "        - idx_to_word: mapping from index to word\n",
        "        Returns:\n",
        "        - decoded: list of decoded sentences\n",
        "        '''\n",
        "        singleton = False\n",
        "        if tokens.ndim == 1:\n",
        "            singleton = True\n",
        "            tokens = tokens[None]\n",
        "        decoded = []\n",
        "        N, T = tokens.shape\n",
        "        for i in range(N):\n",
        "            words = []\n",
        "            for t in range(T):\n",
        "                word = idx_to_word[tokens[i, t]]\n",
        "                if word != '<pad>':\n",
        "                    words.append(word)\n",
        "                if word == '<end>':\n",
        "                    break\n",
        "            decoded.append(' '.join(words))\n",
        "        if singleton:\n",
        "            decoded = decoded[0]\n",
        "        return decoded\n",
        "\n",
        "    def remove_special_tokens(self,caption_text):\n",
        "        return ' '.join([x for x in caption_text.split(' ')\n",
        "                 if ('<end>' not in x and '<start>' not in x and '<unk>' not in x)])\n",
        "    \n",
        "    \n",
        "    \n",
        "    def sample(self, features, max_length=40, b_size=3, model_mode='nic', search_mode='greedy'):\n",
        "        \n",
        "        self.model.eval()\n",
        "        N = features.size(0)\n",
        "\n",
        "        if model_mode == 'nic':\n",
        "\n",
        "            # prepare model input\n",
        "            features = features.to(device=device)\n",
        "            features = self.model.dropout(self.model.bn_f(features))\n",
        "            feeds = torch.ones((N, 1)) * self._start                                       # initial feed, (N, 1)\n",
        "            feeds = feeds.to(device=device, dtype=torch.long)\n",
        "\n",
        "            im_hid = self.model.dropout(self.model.relu(self.model.proj_h(features))).unsqueeze(0)\n",
        "            im_state = self.model.dropout(self.model.relu(self.model.proj_c(features))).unsqueeze(0)\n",
        "            h0 = torch.cat((im_hid, ) * self.model.num_layers, 0)                          # h0, c0: (L, N, H)\n",
        "            #h0 = torch.zeros((self.model.num_layers, N, self.model.hidden_dim)).to(device=device)\n",
        "            c0 = torch.cat((im_state, ) * self.model.num_layers, 0)\n",
        "            #c0 = torch.zeros(h0.size()).to(device=device)\n",
        "            hidden = (h0, c0)\n",
        "\n",
        "            #features = self.model.dropout(self.model.relu(self.model.proj_f(features))).unsqueeze(1)\n",
        "            #_, hidden = self.model.rnn.lstm(features, hidden)\n",
        "\n",
        "            if search_mode == 'greedy':\n",
        "                captions = self._pad * np.ones((N, max_length), dtype=np.int32)\n",
        "                captions[:, 0] = np.ones(N) * self._start\n",
        "                for t in range(1, max_length):\n",
        "                    #word_scores, hidden = self.model.rnn(word_seq=feeds, im_feat=features,     # word_scores: (N, 1, V)\n",
        "                    #                                     hidden=hidden)\n",
        "                   # word_scores, hidden = self.model.lst(word_seq=feeds, hidden=hidden)\n",
        "                   # feeds = torch.argmax(word_scores.squeeze(1), dim=1, keepdim=True)          # feeds: (N, 1)\n",
        "                   # captions[:, t] = feeds.data.cpu().numpy().ravel()\n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    emb = self.model.embed(feeds)\n",
        "                    emb = self.model.dropout_emb(emb)\n",
        "                    word_scores, hidden = self.model.lstm(emb, hidden)\n",
        "                    word_scores = self.model.linear(word_scores)\n",
        "                    feeds = torch.argmax(word_scores.squeeze(1), dim=1, keepdim=True)\n",
        "                    captions[:, t] = feeds.data.cpu().numpy().ravel()\n",
        "        return captions"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBIpMGLGhntM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(im):\n",
        "    rgb_mean = [0.485, 0.456, 0.406]\n",
        "    rgb_std = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    transform = T.Compose([\n",
        "                    T.Resize((224, 224)),\n",
        "                    T.ToTensor(),\n",
        "                    T.Normalize(rgb_mean, rgb_std),\n",
        "                ])\n",
        "    return transform(im).unsqueeze(0)\n",
        "\n",
        "\n",
        "def get_captions(path):\n",
        "    raw_f = open(path + 'Flickr8k.token.txt', 'r').read().strip().split('\\n')\n",
        "    raw_captions = {}\n",
        "    for line in raw_f:\n",
        "        line = line.split('\\t')\n",
        "        im_id, cap = line[0][:len(line[0])-2], line[1]\n",
        "        if im_id not in raw_captions:\n",
        "            raw_captions[im_id] = ['<start> ' + cap + ' <end>']\n",
        "        else:\n",
        "            raw_captions[im_id].append('<start> ' + cap + ' <end>')\n",
        "    return raw_captions\n",
        "\n",
        "\n",
        "def sample_and_plot(im_id):\n",
        "    #for s in raw_captions[im_id]:\n",
        "       # print(s)\n",
        "   # print()\n",
        "    encoder = Encoder_Resnet101()\n",
        "    model = Train_Model(loader)\n",
        "    im = Image.open('K://Learning & Sharing//AI//Udemy//the-complete-neural-networks-bootcamp-theory-applications//FLICKR8K//data//Flickr8k_Dataset//Flicker8k_Dataset//'+im_id)\n",
        "    im_t = preprocess(im).to(device)\n",
        "    feature = encoder(im_t).squeeze().unsqueeze(0)\n",
        "\n",
        "    s1 = model.sample(feature, search_mode='greedy')\n",
        "    s1 = model.decode_captions(s1, loader.dataset.idx_to_word)\n",
        "   # s2 = solver.sample(feature, search_mode='beam', b_size=5)\n",
        "    #s2 = decode_captions(s2, data.idx_to_word)\n",
        "   # s3 = solver.sample(feature, search_mode='beam', b_size=10)\n",
        "   # s3 = decode_captions(s3, data.idx_to_word)\n",
        "   # s4 = solver.sample(feature, search_mode='beam', b_size=15)\n",
        "   # s4 = decode_captions(s4, data.idx_to_word)\n",
        "\n",
        "    print('Greedy:', s1[0])\n",
        "   # print('B=5:', s2[0])\n",
        "   # print('B=10:', s3[0])\n",
        "   # print('B=15:', s4[0])\n",
        "    plt.imshow(np.array(im))\n",
        "    plt.axis('off')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIzvrme2hzs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Several functions adapted from\n",
        "\n",
        "https://github.com/tylin/coco-caption/tree/master/pycocoevalcap/bleu\n",
        "https://github.com/danieljl/keras-image-captioning/blob/master/keras_image_captioning/preprocessors.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "def BLEU_score(gt_caption, sample_caption):\n",
        "    \"\"\"\n",
        "    gt_caption: string, ground-truth caption\n",
        "    sample_caption: string, your model's predicted caption\n",
        "    Returns unigram BLEU score.\n",
        "    \"\"\"\n",
        "    reference = [x for x in gt_caption.split(' ')\n",
        "                 if ('<end>' not in x and '<start>' not in x and '<unk>' not in x)]\n",
        "    hypothesis = [x for x in sample_caption.split(' ')\n",
        "                  if ('<end>' not in x and '<start>' not in x and '<unk>' not in x)]\n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
        "    return BLEUscore\n",
        "\n",
        "\"\"\"\n",
        "For the following functions.\n",
        "\n",
        "Copyright (c) 2015, Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "1. Redistributions of source code must retain the above copyright notice, this\n",
        "   list of conditions and the following disclaimer.\n",
        "2. Redistributions in binary form must reproduce the above copyright notice,\n",
        "   this list of conditions and the following disclaimer in the documentation\n",
        "   and/or other materials provided with the distribution.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
        "ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "The views and conclusions contained in the software and documentation are those\n",
        "of the authors and should not be interpreted as representing official policies,\n",
        "either expressed or implied, of the FreeBSD Project.\n",
        "\"\"\"\n",
        "\n",
        "def remove_special_tokens(caption_text):\n",
        "    return ' '.join([x for x in caption_text.split(' ')\n",
        "             if ('<end>' not in x and '<start>' not in x and '<unk>' not in x)])\n",
        "\n",
        "def precook(s, n=4, out=False):\n",
        "    \"\"\"Takes a string as input and returns an object that can be given to\n",
        "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
        "    can take string arguments as well.\"\"\"\n",
        "    words = s.split()\n",
        "    counts = defaultdict(int)\n",
        "    for k in range(1,n+1):\n",
        "        for i in range(len(words)-k+1):\n",
        "            ngram = tuple(words[i:i+k])\n",
        "            counts[ngram] += 1\n",
        "    return (len(words), counts)\n",
        "\n",
        "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    reflen = []\n",
        "    maxcounts = {}\n",
        "    for ref in refs:\n",
        "        rl, counts = precook(ref, n)\n",
        "        reflen.append(rl)\n",
        "        for (ngram,count) in counts.items():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "    if eff == \"shortest\":\n",
        "        reflen = min(reflen)\n",
        "    elif eff == \"average\":\n",
        "        reflen = float(sum(reflen))/len(reflen)\n",
        "\n",
        "    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n",
        "\n",
        "    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n",
        "\n",
        "    return (reflen, maxcounts)\n",
        "\n",
        "def cook_test(test, crefs_from_cook, eff=None, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    reflen, refmaxcounts = crefs_from_cook\n",
        "    testlen, counts = precook(test, n, True)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff == \"closest\":\n",
        "        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n",
        "    else: ## i.e., \"average\" or \"shortest\" or None\n",
        "        result[\"reflen\"] = reflen\n",
        "\n",
        "    result[\"testlen\"] = testlen\n",
        "\n",
        "    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n",
        "\n",
        "    result['correct'] = [0]*n\n",
        "    for (ngram, count) in counts.items():\n",
        "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
        "\n",
        "    return result\n",
        "\n",
        "class BleuScorer(object):\n",
        "    \"\"\"Bleu scorer.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n",
        "    # special_reflen is used in oracle (proportional effective ref len for a node).\n",
        "\n",
        "    def copy(self):\n",
        "        ''' copy the refs.'''\n",
        "        new = BleuScorer(n=self.n)\n",
        "        new.ctest = copy.copy(self.ctest)\n",
        "        new.crefs = copy.copy(self.crefs)\n",
        "        new._score = None\n",
        "        return new\n",
        "\n",
        "    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n",
        "        ''' singular instance '''\n",
        "\n",
        "        self.n = n\n",
        "        self.crefs = []\n",
        "        self.ctest = []\n",
        "        self.cook_append(test, refs)\n",
        "        self.special_reflen = special_reflen\n",
        "\n",
        "    def cook_append(self, test, refs):\n",
        "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
        "\n",
        "        if refs is not None:\n",
        "            self.crefs.append(cook_refs(refs))\n",
        "            if test is not None:\n",
        "                cooked_test = cook_test(test, self.crefs[-1])\n",
        "                self.ctest.append(cooked_test) ## N.B.: -1\n",
        "            else:\n",
        "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
        "\n",
        "        self._score = None ## need to recompute\n",
        "\n",
        "    def ratio(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._ratio\n",
        "\n",
        "    def score_ratio(self, option=None):\n",
        "        '''return (bleu, len_ratio) pair'''\n",
        "        return (self.fscore(option=option), self.ratio(option=option))\n",
        "\n",
        "    def score_ratio_str(self, option=None):\n",
        "        return \"%.4f (%.2f)\" % self.score_ratio(option)\n",
        "\n",
        "    def reflen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._reflen\n",
        "\n",
        "    def testlen(self, option=None):\n",
        "        self.compute_score(option=option)\n",
        "        return self._testlen\n",
        "\n",
        "    def retest(self, new_test):\n",
        "        if type(new_test) is str:\n",
        "            new_test = [new_test]\n",
        "        assert len(new_test) == len(self.crefs), new_test\n",
        "        self.ctest = []\n",
        "        for t, rs in zip(new_test, self.crefs):\n",
        "            self.ctest.append(cook_test(t, rs))\n",
        "        self._score = None\n",
        "\n",
        "        return self\n",
        "\n",
        "    def rescore(self, new_test):\n",
        "        ''' replace test(s) with new test(s), and returns the new score.'''\n",
        "\n",
        "        return self.retest(new_test).compute_score()\n",
        "\n",
        "    def size(self):\n",
        "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
        "        return len(self.crefs)\n",
        "\n",
        "    def __iadd__(self, other):\n",
        "        '''add an instance (e.g., from another sentence).'''\n",
        "\n",
        "        if type(other) is tuple:\n",
        "            ## avoid creating new BleuScorer instances\n",
        "            self.cook_append(other[0], other[1])\n",
        "        else:\n",
        "            assert self.compatible(other), \"incompatible BLEUs.\"\n",
        "            self.ctest.extend(other.ctest)\n",
        "            self.crefs.extend(other.crefs)\n",
        "            self._score = None ## need to recompute\n",
        "\n",
        "        return self\n",
        "\n",
        "    def compatible(self, other):\n",
        "        return isinstance(other, BleuScorer) and self.n == other.n\n",
        "\n",
        "    def single_reflen(self, option=\"average\"):\n",
        "        return self._single_reflen(self.crefs[0][0], option)\n",
        "\n",
        "    def _single_reflen(self, reflens, option=None, testlen=None):\n",
        "\n",
        "        if option == \"shortest\":\n",
        "            reflen = min(reflens)\n",
        "        elif option == \"average\":\n",
        "            reflen = float(sum(reflens))/len(reflens)\n",
        "        elif option == \"closest\":\n",
        "            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n",
        "        else:\n",
        "            assert False, \"unsupported reflen option %s\" % option\n",
        "\n",
        "        return reflen\n",
        "\n",
        "    def recompute_score(self, option=None, verbose=0):\n",
        "        self._score = None\n",
        "        return self.compute_score(option, verbose)\n",
        "\n",
        "    def compute_score(self, option=None, verbose=0):\n",
        "        n = self.n\n",
        "        small = 1e-9\n",
        "        tiny = 1e-15 ## so that if guess is 0 still return 0\n",
        "        bleu_list = [[] for _ in range(n)]\n",
        "\n",
        "        if self._score is not None:\n",
        "            return self._score\n",
        "\n",
        "        if option is None:\n",
        "            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n",
        "\n",
        "        self._testlen = 0\n",
        "        self._reflen = 0\n",
        "        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
        "\n",
        "        # for each sentence\n",
        "        for comps in self.ctest:\n",
        "            testlen = comps['testlen']\n",
        "            self._testlen += testlen\n",
        "\n",
        "            if self.special_reflen is None: ## need computation\n",
        "                reflen = self._single_reflen(comps['reflen'], option, testlen)\n",
        "            else:\n",
        "                reflen = self.special_reflen\n",
        "\n",
        "            self._reflen += reflen\n",
        "\n",
        "            for key in ['guess','correct']:\n",
        "                for k in range(n):\n",
        "                    totalcomps[key][k] += comps[key][k]\n",
        "\n",
        "            # append per image bleu score\n",
        "            bleu = 1.\n",
        "            for k in range(n):\n",
        "                bleu *= (float(comps['correct'][k]) + tiny) \\\n",
        "                        /(float(comps['guess'][k]) + small)\n",
        "                bleu_list[k].append(bleu ** (1./(k+1)))\n",
        "            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n",
        "            if ratio < 1:\n",
        "                for k in range(n):\n",
        "                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "            if verbose > 1:\n",
        "                print(comps, reflen)\n",
        "\n",
        "        totalcomps['reflen'] = self._reflen\n",
        "        totalcomps['testlen'] = self._testlen\n",
        "\n",
        "        bleus = []\n",
        "        bleu = 1.\n",
        "        for k in range(n):\n",
        "            bleu *= float(totalcomps['correct'][k] + tiny) \\\n",
        "                    / (totalcomps['guess'][k] + small)\n",
        "            bleus.append(bleu ** (1./(k+1)))\n",
        "        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n",
        "        if ratio < 1:\n",
        "            for k in range(n):\n",
        "                bleus[k] *= math.exp(1 - 1/ratio)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(totalcomps)\n",
        "            print(\"ratio:\", ratio)\n",
        "\n",
        "        self._score = bleus\n",
        "        return self._score, bleu_list\n",
        "\n",
        "class Bleu:\n",
        "    def __init__(self, n=4):\n",
        "        # default compute Blue score up to 4\n",
        "        self._n = n\n",
        "        self._hypo_for_image = {}\n",
        "        self.ref_for_image = {}\n",
        "\n",
        "    def compute_score(self, gts, res):\n",
        "\n",
        "        assert(type(gts) is list)\n",
        "        assert(type(res) is list)\n",
        "\n",
        "        bleu_scorer = BleuScorer(n=self._n)\n",
        "        for hypo, ref in zip(res, gts):\n",
        "            # Sanity check.\n",
        "            assert(type(hypo) is str)\n",
        "            assert(type(ref) is str)\n",
        "\n",
        "            bleu_scorer += (hypo, [ref])  # (string, list)\n",
        "\n",
        "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
        "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
        "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
        "\n",
        "        # return (bleu, bleu_info)\n",
        "        return score, scores\n",
        "\n",
        "    def method(self):\n",
        "        return \"Bleu\"\n",
        "\n",
        "\"\"\"\n",
        "For the following functions.\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2017–2018 Daniel Julius Lasiman\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\"\"\"\n",
        "\n",
        "class Score(object):\n",
        "    \"\"\"A subclass of this class is an adapter of pycocoevalcap.\"\"\"\n",
        "\n",
        "    def __init__(self, score_name, implementation):\n",
        "        self._score_name = score_name\n",
        "        self._implementation = implementation\n",
        "\n",
        "    def calculate(self, references, predictions):\n",
        "        avg_score, scores = self._implementation.compute_score(\n",
        "                                            references, predictions)\n",
        "        if isinstance(avg_score, (list, tuple)):\n",
        "            avg_score = map(float, avg_score)\n",
        "        else:\n",
        "            avg_score = float(avg_score)\n",
        "        return {self._score_name: avg_score}\n",
        "\n",
        "class BLEU(Score):\n",
        "    def __init__(self, n=4):\n",
        "        implementation = Bleu(n)\n",
        "        super(BLEU, self).__init__('bleu', implementation)\n",
        "        self._n = n\n",
        "\n",
        "    def calculate(self, references, predictions):\n",
        "        \"\"\"\n",
        "        Input: two lists of strings\n",
        "        Return: dict of bleus (bleu_1 to bleu_4)\n",
        "        \"\"\"\n",
        "        name_to_score = super(BLEU, self).calculate(references,\n",
        "                                                    predictions)\n",
        "        scores = list(name_to_score.values())[0]\n",
        "        result = {}\n",
        "        for i, score in enumerate(scores, start=1):\n",
        "            name = '{}_{}'.format(self._score_name, i)\n",
        "            result[name] = score\n",
        "        return result\n",
        "\n",
        "eval_bleu = BLEU()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecsBC44siIsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "3d446e1b-f416-4bec-becd-137ea899c740"
      },
      "source": [
        "#from data_utils import prepare_loader\n",
        "#image_path = 'K://Learning & Sharing//AI//Udemy//the-complete-neural-networks-bootcamp-theory-applications//FLICKR8K//data//Flickr8k_Dataset//'\n",
        "\n",
        "# Setting GPU / CPU as device\n",
        "if  torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "####################Variables###########\n",
        "batch_size = 100\n",
        "epochs = 15\n",
        "step_size = 16\n",
        "lr_init = 0.01\n",
        "lr_decay = 0.01\n",
        "print_every = 1\n",
        "vocab_size = 9080\n",
        "path = '/content/drive/My Drive/Colab Projects/RNN/data/FLICKR8K/images/'\n",
        "data_part = 'train'\n",
        "transforms = True\n",
        "########################################\n",
        "\n",
        "\n",
        "data_preproc = Data_Preprocessing(path)\n",
        "loader = Data_Loader(data_preporc=data_preproc,data_part=data_part,transforms=transforms)\n",
        "loader = loader.prepare_loader(batch_size)\n",
        "\n",
        "train = Train_Model(loader)\n",
        "train.train(epochs,batch_size)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 / 2:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "t :0 , loaders.image shape: torch.Size([100, 3, 224, 224]) dataloaders.captions shape:torch.Size([100, 40])\n",
            "mask shape: torch.Size([3900])\n",
            "cap input shape: torch.Size([100, 39])\n",
            "cap target shape: torch.Size([100, 39])\n",
            "scores shape: torch.Size([100, 39, 9080]) hidden shape: torch.Size([1, 100, 160])\n",
            "t = 1, loss = 117.6873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "t :1 , loaders.image shape: torch.Size([100, 3, 224, 224]) dataloaders.captions shape:torch.Size([100, 40])\n",
            "mask shape: torch.Size([3900])\n",
            "cap input shape: torch.Size([100, 39])\n",
            "cap target shape: torch.Size([100, 39])\n",
            "scores shape: torch.Size([100, 39, 9080]) hidden shape: torch.Size([1, 100, 160])\n",
            "t = 2, loss = 114.7360\n",
            "\n",
            "Epoch 2 / 2:\n",
            "t :0 , loaders.image shape: torch.Size([100, 3, 224, 224]) dataloaders.captions shape:torch.Size([100, 40])\n",
            "mask shape: torch.Size([3900])\n",
            "cap input shape: torch.Size([100, 39])\n",
            "cap target shape: torch.Size([100, 39])\n",
            "scores shape: torch.Size([100, 39, 9080]) hidden shape: torch.Size([1, 100, 160])\n",
            "t = 1, loss = 116.2263\n",
            "t :1 , loaders.image shape: torch.Size([100, 3, 224, 224]) dataloaders.captions shape:torch.Size([100, 40])\n",
            "mask shape: torch.Size([3900])\n",
            "cap input shape: torch.Size([100, 39])\n",
            "cap target shape: torch.Size([100, 39])\n",
            "scores shape: torch.Size([100, 39, 9080]) hidden shape: torch.Size([1, 100, 160])\n",
            "t = 2, loss = 127.3631\n",
            "{\"metric\": \"Train Loss\", \"value\": 0.6870, \"epoch\": 2}\n",
            "Training complete in 1m 59s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF1YaeSkjXHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}